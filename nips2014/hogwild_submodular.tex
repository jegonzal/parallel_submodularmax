
\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}


% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{float}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{wrapfig}
\setlength{\emergencystretch}{3em}
\usepackage[numbers]{natbib}

\usepackage{multirow}

% For algorithms
\usepackage[algoruled,vlined,linesnumbered]{algorithm2e}
%\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{multicol}
\usepackage{comment}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\graphicspath{{figures/}}


\newcommand{\ie}{{\em i.e.,}~}
\newcommand{\eg}{{\em e.g.,}~}

\begingroup
    \makeatletter
    \@for\theoremstyle:=definition,remark,plain\do{%
        \expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
            \addtolength\thm@preskip\parskip
            }%
        }
\endgroup
\newtheorem{dfn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{claim}{Claim}

\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Program}

\newenvironment{denseitemize}{
\begin{itemize}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}




\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
%  numbers=left,
  morestring=[b]"""
}



% Commenting system
\newcommand{\Comments}{1}
\newcommand{\note}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\taunghao}[1]{\note{red}{[XP: #1]}}
\newcommand{\joey}[1]{\note{blue}{[JG: #1]}}
\newcommand{\stef}[1]{\note{green}{[SJ: #1]}}
\newcommand{\tb}[1]{\note{cyan}{[TB: #1]}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}






%% ---------------------------------------------------------
%% Terminology
\newcommand{\term}[1]{\textbf{#1}}


%% ---------------------------------------------------------
%% Citation/Reference commands
\newcommand{\citecf}[1]{(\cf, \cite{#1})}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\listref}[1]{Listing~\ref{#1}}

\newcommand{\eqnref}[1]{Eq.~(\ref{#1})}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}

\newcommand{\dfnref}[1]{Definition~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\propref}[1]{Prop.~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\exmpref}[1]{Example~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\algref}[1]{Alg.~\ref{#1}}
\newcommand{\procref}[1]{Proc.~\ref{#1}}
\newcommand{\alglineref}[1]{Line~\ref{#1}}
\newcommand{\probref}[1]{Problem~(\ref{#1})}
\newcommand{\appendref}[1]{Appendix~\ref{#1}}

%% ---------------------------------------------------------
%% Basic Math
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}


%% ---------------------------------------------------------
%% special math functions
\newcommand{\polylog}[2]{\,\mathbf{Li}_{#1}\left( #2 \right)}
\newcommand{\harmonic}[2]{\,\mathbf{h}_{#1}\left( #2 \right)}

%% ---------------------------------------------------------
%% Norms
\newcommand{\Lone}{L_{1}}
\newcommand{\Linf}{L_{\infty}}
\newcommand{\LInfNorm}[1]{\left|\left| #1 \right|\right|_{\infty}}
\newcommand{\LOneNorm}[1]{\left|\left| #1 \right|\right|_1}



%% ---------------------------------------------------------
%% Probability notation
\newcommand{\given}{\,|\,}
\newcommand{\stdist}[1]{\mathbf{\pi} \left( #1 \right) }
\newcommand{\Prb}[1]{\mathbf{P} \left( #1 \right) }
\newcommand{\PrbEst}[1]{\mathbf{\tilde{P}} \left( #1 \right) }
\newcommand{\Ent}[1]{\mathbf{H} \left( #1 \right) }
\newcommand{\PiPrb}[1]{\Prb{ #1 } }
\newcommand{\Kern}[1]{K \left( #1 \right) }
\newcommand{\Ex}[1]{\mathbf{E} \left[ #1 \right] }
\newcommand{\Exwrt}[2]{\mathbf{E}_{#1} \left[ #2 \right] }
\newcommand{\Variance}[1]{\mathbf{Var} \left[ #1 \right] }
\newcommand{\Ind}[1]{\mathbf{1}\left[ #1 \right]}
\newcommand{\Bern}[1]{\text{Bern}( #1 ) }

%% ---------------------------------------------------------
%% Set notation
\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\vecspace}{\mathcal{V}}
\newcommand{\Union}{\bigcup}
\newcommand{\Inter}{\bigcap}
\newcommand{\union}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\size}[1]{\left| #1 \right|}


%% ---------------------------------------------------------
%% Complexity
\newcommand{\BigO}[1]{O\hspace{-1pt}\left( #1 \right)}
\newcommand{\BigTheta}[1]{\Theta \left( #1 \right)}
\newcommand{\BigOmega}[1]{\Omega \left( #1 \right)}





%% ---------------------------------------------------------
%% Algorithms
\SetKwFor{ParForAll}{for}{do in parallel}{end}
\SetKwFunction{Map}{Map}
\SetKwFunction{Reduce}{Reduce}

\SetKwInput{Input}{Input}
\SetKwInput{Output}{Output}
\SetKwInput{SideEffect}{SideEffect}
\SetKwInput{Define}{Define}
\SetKwInput{Global}{Global}
\SetKwFor{DoWithProbability}{with probability}{}{}
\SetKwFunction{DPMeansOp}{DPMeansOp}
\SetKwFunction{DPValidate}{DPValidate}
\SetKwFunction{OFLValidate}{OFLValidate}
\SetKwFunction{BPMeansOp}{BPMeansOp}
\SetKwFunction{BPValidate}{BPValidate}
\SetKwFunction{NewClusters}{AcceptedClusters}

\SetKw{WaitUntil}{wait until}

\SetKwFunction{Mean}{Mean}
\SetKwFunction{Ref}{Ref}


%% ---------------------------------------------------------
%% Paper specific notation

% All the data
\newcommand{\data}{\mathcal{D}}
% \datablock{machine}
\newcommand{\datablock}[1]{\data_{#1}}

\newcommand{\clusters}{\mathcal{C}}
\newcommand{\gclusters}{\hat\clusters}
\newcommand{\newclusters}{\tilde\clusters}
% local clusters \lclusters{machine}
\newcommand{\lclusters}[1]{\clusters_{#1}}

%\newcommand{\bregd}[2]{D_\phi\left(#1,#2\right)}
\newcommand{\bregd}[2]{\left\|#1-#2\right\|}


% for ofl analysis:
\newcommand{\CFL}{C^{\text{FL}}}
\newcommand{\muFL}{\mu^{\text{FL}}}


\title{Parallel Double Greedy Submodular Maximization}

\author{
Xinghao Pan$^1$ Joseph Gonzalez$^1$ Stefanie Jegelka$^1$ Joseph Bradley$^{1}$ Michael I. Jordan$^{1,2}$\\
$^1$Department of Electrical Engineering and Computer Science, and $^2$Department of Statistics\\
University of California, Berkeley\\
Berkeley, CA USA 94720\\
  \texttt{\{xinghao,jegonzal,stefje,tab,?\}@eecs.berkeley.edu} \\
}

% \address{University of California
% 465 Soda Hall, MC-1776
% Berkeley, CA 94720-1776}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


%\nipsfinalcopy

\begin{document}


\maketitle


\begin{abstract}
Bidirectional greedy is a sequential algorithm that does not scale well to problems of large scale.
We present 2 approaches to extend the bidirectional greedy algorithm to a parallel setting.
The first, `hogwild' approach emphasizes speed at the cost of worsening the approximation by an additive factor;
the second approach guarantees the same approximation bound by sacrificing concurrency.
\end{abstract}

\section{Introduction}
The bidirectional greedy algorithm \cite{buchbinder2012} gives an approximation of $E[F(A)] = 1/2 f(OPT)$, where $A$ is the algorithm output, and $OPT$ is an optimal solution.

The hogwild algorithm can give an approximation of $E[F(A)] = \frac{1}{2} F(OPT) - \frac{1}{4}\sum_iE[\rho_i]$, where $\rho_i$ is the maximum difference in the marginal gain that may result from not knowing the full information when deciding whether to include or exclude element $i$.

The OCC algorithm \taunghao{for the lack of a better name} guarantees an outcome that is equivalent to a sequential run of the bidirectional greedy algorithm.
Theoretical properties of the bidirectional greedy algorithm immediately translates to the OCC algorithm -- in particular, the OCC algorithm gives the same approximation factor of $1/$.
In contrast to the hogwild approach, OCC introduces more coordination and thus provides less concurrency.











\section{Submodular maximization}

The sequential bidirectional greedy \cite{buchbinder2012} algorithm monotonically grows $A^i$ and shrinks $B^i$.









\section{Approaches for parallel learning}

Two approaches that allow us to trade off speed with approximation guarantees.

\subsection{Coordination free}
Simply run everything in parallel.
Optimized for speed, but does not necessarily provide the correct answer.
Requires work to prove correctness.

\subsection{Concurrency control}
Ensures `serial equivalence' -- the outcome of the parallel algorithm is equivalent to some execution of the sequential algorithm.
Locally, threads take actions that are guaranteed to be safe (i.e. preserves serial equivalence), and forces additional coordination only when they are unable to execute their action safely.
Designed for correctness, but requires coordination that compromises speed.
Work is only required to demonstrate that coordination is limited.


\begin{figure}[h]
  \footnotesize
  \centering
  \begin{multicols}{2}
    \begin{minipage}{0.49\textwidth}
      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Serial submodular maximization}
        \label{alg:submax}
        %\Input{}
        $A^0 = \emptyset$, $B^0 = V$\;
        \For{$i = 1$ to $n$}{
          $\Delta_{+}(i) = F(A^{i-1}\cup i) - F(A^{i-1})$\;
          $\Delta_{-}(i) = F(B^{i-1}\backslash i) - F(B^{i-1})$\;
          Draw $u_i\sim Unif(0,1)$\;
          \If {$u_i<\frac{[\Delta_{+}(i)]_+}{[\Delta_{+}(i)]_+ + [\Delta_{-}(i)]_+}$}{
            $A^i := A^{i-1} \cup i$;\\
            $B^i := B^{i-1}$\;
          }\Else{
            $A^i := A^{i-1}$;\\
            $B^i := B^{i-1}\backslash i$\;
          }
        }
        %\Output{$A_n$}
      \end{algorithm}
      
      
      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Hogwild bidirectional greedy}
        \label{alg:hogwild}
        \lFor{$e\in V$}{$\hat{A}(e) = 0$, $\hat{B}(e) = 1$}\;
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $\Delta_{+}^{\max}(e) = F(\hat{A}\cup e) - F(\hat{A})$\;\label{alg:hogwild:deltaadd}
            $\Delta_{-}^{\max}(e) = F(\hat{B}\backslash e) - F(\hat{B})$\;\label{alg:hogwild:deltarem}
            Draw $u_e\sim Unif(0,1)$\;\label{alg:hogwild:time}
            \If {$u_e<\frac{[\Delta_{+}^{\max}(e)]_+}{[\Delta_{+}^{\min}(e)]_+ + [\Delta_{-}^{\max}(e)]_+}$}{
              $\hat{A}(e) \leftarrow 1$\;\label{alg:hogwild:add}
            }\lElse{
              $\hat{B}(e) \leftarrow 0$\;\label{alg:hogwild:rem}
            }
          }
        }
      \end{algorithm}


      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Hogwild for separable sums}
        \label{alg:hogwildsepsum}
        \lFor{$e\in V$}{$\hat{A}(e) = 0$}\;
        \lFor{$l = 1,\dots,L$}{$\hat\alpha_l = 0$, $\hat\beta_l = \sum_{e\in S_l}w_l(e)$}\;
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $\Delta_+^{\max}(e) = -\lambda v(e) + \sum_{S_l\ni e} g(\hat\alpha_l + w_l(e)) - g(\hat\alpha_l)$\;
            $\Delta_-^{\max}(e) = +\lambda v(e) + \sum_{S_l\ni e} g(\hat\beta_l   - w_l(e)) - g(\hat\beta_l)$\;
            Draw $u_e\sim Unif(0,1)$\;
            \If {$u_e<\frac{[\Delta_{+}^{\max}(e)]_+}{[\Delta_{+}^{\min}(e)]_+ + [\Delta_{-}^{\max}(e)]_+}$}{
              $\hat{A}(e) \leftarrow 1$\;
              \lFor{$l: e \in S_l$}{
                $\hat\alpha_l \leftarrow \hat\alpha_l + w_l(e)$
              }
            }\lElse{
              \lFor{$l: e \in S_l$}{
                $\hat\beta_l \leftarrow \hat\beta_l - w_l(e)$
              }
            }
          }
        }
      \end{algorithm}
      
    \end{minipage}

    \begin{minipage}{0.49\textwidth}
\begin{algorithm}[H]
\DontPrintSemicolon
\caption{OCC bidirectional greedy}
\label{alg:occ}
\lFor{$e\in V$}{$\hat{A}(e) = \tilde{A}(e) = 0$, $\hat{B}(e) = \tilde{B}(e) = 1$}\;
\lFor{$i = 1,\dots,|V|$}{result$(i) = 0$}\;
\lFor{$i = 1,\dots,|V|$}{processed$(i) = false$}\;
$\iota = 0$\;
\ParForAll{$p \in \set{1, \ldots, P}$}{
  \While{$\exists$ element to process}{
    $e = $ next element to process\;
    $\tilde{A}(e) \leftarrow 1$\;
    $\tilde{B}(e) \leftarrow 0$\;
    $i = \iota$; $\iota \leftarrow \iota + 1$\;\label{alg:occ:time}
    $\Delta_+^{\min}(e) = F(\tilde{A} \cup e) - F(\tilde{A})$\;
    $\Delta_+^{\max}(e) = F(\hat{A}   \cup e) - F(\hat{A})$\;
    $\Delta_-^{\min}(e) = F(\tilde{B} \backslash e) - F(\tilde{B})$\;
    $\Delta_-^{\max}(e) = F(\hat{B}   \backslash e) - F(\hat{B})$\;
    Draw $u_e \sim Unif(0,1)$\;
    \If {$u_e < \frac{[\Delta_+^{\min}(e)]_+}{[\Delta_+^{\min}(e)]_+ + [\Delta_-^{\max}(e)]_+}$}{
      result$(i) \leftarrow 1$\;
    }\ElseIf {$u_e > \frac{[\Delta_+^{\max}(e)]_+}{[\Delta_+^{\max}(e)]_+ + [\Delta_-^{\min}(e)]_+}$}{
      result$(i) \leftarrow -1$\;
    }
    \WaitUntil $\forall j<i$, result$(j) \neq 0$\;
    \lIf {result$(i) = 0$}{validate($p$, $e$, $i$)}\;
    \If {result$(i) = 1$}{
      $\hat{A}(e)   \leftarrow 1$\;
      $\tilde{B}(e) \leftarrow 1$\;
    }\Else{
      $\tilde{A}(e) \leftarrow 0$\;
      $\hat{B}(e)   \leftarrow 0$\;
    }
    processed$(i) = true$\;
  }
}
\end{algorithm}

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{validate($p$, $e$, $i$)}
\label{alg:occvalidate}
  \WaitUntil $\forall j < i$, processed$(j) = true$\;
  $\Delta_+(e) = F(\hat{A} \cup e) - F(\hat{A})$\;
  $\Delta_-(e) = F(\hat{B} \backslash e) - F(\hat{B})$\;
  \lIf {$u_e < \frac{[\Delta_+(e)]_+}{[\Delta_+(e)]_+ + [\Delta_-(e)]_+}$}{
    result$(i) \leftarrow 1$\;
  }\lElse{
    result$(i) \leftarrow -1$\;
  }
\end{algorithm}


  \label{fig:submax}
    \end{minipage}
    
    
    
  \end{multicols}
\end{figure}








\section{Hogwild for arbitrary submodular $F$}
Algorithm \ref{alg:hogwild} is the hogwild parallel bidirectional greedy unconstrained submodular maximization algorithm.
We associate with each element $e$ a time $T_e$ at which Algorithm \ref{alg:hogwild} line \ref{alg:hogwild:time} is executed, and order the elements according to the times $T_e$.
Let $\iota(e)$ be the position of $e$ in this ordering.
This total ordering on elements also allows us to define sets $A^i$, $B^i$ corresponding to that obtained by the serial algorithm;
specifically, $A^i = \{e' : e' \in A, \iota(e') < i\}$ and $B^i = A^i \cup \{e': \iota(e') \geq i\}$.

Note that in Algorithm \ref{alg:hogwild}, lines \ref{alg:hogwild:deltaadd} and \ref{alg:hogwild:deltarem} may be executed in parallel with lines \ref{alg:hogwild:add} and \ref{alg:hogwild:rem}.
Hence, $\Delta_+^{\max}(e)$ and $\Delta_-^{\max}(e)$ (lines \ref{alg:hogwild:deltaadd} and \ref{alg:hogwild:deltarem}) may be computed with different values of $\hat{A}(e')$.
We denote by $\hat{A}_e$ and $\hat{B}_e$ respectively the vectors of $\hat{A}$ and $\hat{B}$ that are used in the computation of $\Delta_+^{\max}(e)$ and $\Delta_-^{\max}(e)$.

\begin{lem}\label{lem:set_bound} For any $e\in V$, $\hat{A}_e \subseteq A^{\iota(e)-1}$, $\hat{B}_e \supseteq B^{\iota(e)-1}$.
\end{lem}
\begin{proof}
Consider any element $e'\in V$.
If $e'\in \hat{A}_e$, it must be the case that the algorithm set $\hat{A}(e')$ to 1 (line \ref{alg:hogwild:add}) before $T_e$, which implies $\iota(e') < \iota(e)$, and hence $e' \in A^{\iota(e)-1}$.
So $\hat{A}_e \subseteq A^{\iota(e)-1}$.

Similarly, if $e'\not\in \hat{B}_e$, then the algorithm set $\hat{B}(e')$ to 0 (line \ref{alg:hogwild:rem}) before $T_e$, so $\iota(e') < \iota(e)$.
Also, $e'\not\in A$ because the execution of line \ref{alg:hogwild:rem} excludes the execution of line \ref{alg:hogwild:add}.
Therefore, $e'\not\in A^{\iota(e)-1}$, and $e'\not\in B^{\iota(e)-1}$.
So $\hat{B}_e \subseteq B^{\iota(e)-1}$.
\end{proof}

It's easy to see that
\begin{align*}
  \Delta_{+}       (e) &= F(A^{\iota(e)-1}\cup i) - F(A^{\iota(e)-1}),
& \Delta_{-}       (e) &= F(B^{\iota(e)-1}\backslash e) - F(B^{\iota(e)-1})\\
  \Delta_{+}^{\max}(e) &= F(\hat{A}_e\cup e) - F(\hat{A}_e),
& \Delta_{-}^{\max}(e) &= F(\hat{B}_e\backslash e) - F(\hat{B}_e).
\end{align*}

\begin{cor}\label{cor:delta_bound}
Submodularity of $F$ implies that
\begin{align*}
  \Delta_{+}(e) &\quad\leq\quad \Delta_{+}^{\max}(e),
& \Delta_{-}(e) &\quad\leq\quad \Delta_{-}^{\max}(e).
\end{align*}
\end{cor}

\subsection{Hogwild for separable sums $F$}
For some functions $F$, we can maintain sketches / statistics to aid the computation of $\Delta_+^{\max}$, $\Delta_-^{\max}$, and obtain the bounds given in Corollary \ref{cor:delta_bound}.
In particular, we consider functions of the form
\begin{align*}
F(X) = \sum_{l=1}^L g\left(\sum_{i\in X\cup S_l} w_l(i)\right) - \lambda\sum_{i\in X} v(i),
\end{align*}
where $S_l \subseteq V$ are (possibly overlapping) groups of elements in the ground set, $g$ is a non-decreasing concave scalar function, and $w_l(i)$ and $v(i)$ are non-negative scalar weights.
It is easy to see that
\begin{align*}
F(X \cup e) - F(X) = \sum_{l: e\in S_l} \left[g\left(w_l(e) + \sum_{i\in X\cup S_l} w_l(i)\right) - g\left(\sum_{i\in X\cup S_l} w_l(i)\right)\right] - \lambda v(e).
\end{align*}
Define
\begin{align*}
  \hat\alpha_l              &= \sum_{j\in \hat{A}\cup S_l} w_l(j),
& \hat\alpha_{l,e}          &= \sum_{j\in \hat{A}_e\cup S_l} w_l(j),
& \alpha_l^{\iota(e)-1} &= \sum_{j\in A^{\iota(e)-1}\cup S_l} w_l(j).\\
  \hat\beta_l              &= \sum_{j\in \hat{B}\cup S_l} w_l(j),
& \hat\beta_{l,e}          &= \sum_{j\in \hat{B}_e\cup S_l} w_l(j),
& \beta_l^{\iota(e)-1} &= \sum_{j\in B^{\iota(e)-1}\cup S_l} w_l(j).
\end{align*}
We can update $\hat\alpha_l$ and $\hat\beta_l$ according to Algorithm \ref{alg:hogwildsepsum}.
Following arguments analogous to that of Lemma \ref{lem:set_bound}, we can show the following:

\begin{lem} For each $l$ and $e\in V$, $\hat\alpha_{l,e} \leq \alpha_l^{\iota(e)-1}$ and $\hat\beta_{l,e} \geq \beta_l^{\iota(e)-1}$.
\end{lem}

\begin{cor} Concavity of $g$ implies
\begin{align*}
\Delta_+^{\max}(e)
&= \sum_{l:e\in S_l} \left[g(\hat\alpha_{l,e} + w_l(e)) - g(\hat\alpha_{l,e})\right] - \lambda v(e)\\
&\geq \sum_{l:e\in S_l} \left[g(\hat\alpha_l^{\iota(e)-1} + w_l(e)) - g(\hat\alpha_l^{\iota(e)-1})\right] - \lambda v(e)
&= \Delta_+(e),\\
\Delta_-^{\max}(e)
&= \sum_{l:e\in S_l} \left[g(\hat\beta_{l,e} - w_l(e)) - g(\hat\beta_{l,e})\right] + \lambda v(e)\\
&\geq \sum_{l:e\in S_l} \left[g(\hat\beta_l^{\iota(e)-1} - w_l(e)) - g(\hat\beta_l^{\iota(e)-1})\right] + \lambda v(e)
&= \Delta_-(e),
\end{align*}
\end{cor}








\section{Concurrency control}

Algorithm \ref{alg:occ} is the OCC bidirectional greedy algorithm.
Unlike the hogwild algorithm, the OCC algorithm ensures serial equivalence by maintaining four sets $\hat{A}$, $\tilde{A}$, $\hat{B}$, $\tilde{B}$, which serve as upper and lower bounds on $A$ and $B$.
Each thread can determine locally if a decision to include / exclude an element can be taken safely.
Otherwise, the validation process (Algorithm \ref{alg:occvalidate}) waits until it is certain about $A$, $B$ before proceeding.

The serialization order is given by $\iota(e)$, which is the value of $\iota$ at line \ref{alg:occ:time} of Algorithm \ref{alg:occ}.
We will show that the outcome of Algorithm \ref{alg:occ} is equivalent to the sequential algorithm executed with ordering given by $\iota$.

\begin{lem} $\hat{A}_e \subseteq A^{\iota(e)-1} \subseteq \tilde{A}_e \backslash e$, and $\hat{B}_e \supseteq B^{\iota(e)-1} \supseteq \tilde{B}_e \backslash e$.
\end{lem}
\begin{proof}
There are 4 parts to this proof.
\begin{enumerate}
\item $e' \in \hat{A}_e \implies e' \in A^{\iota(e)-1}$.
\item $e' \in A^{\iota(e)-1} \implies e' \in\tilde{A}_e \backslash e$.
\item $e' \not\in \hat{B}_e \implies e' \not\in B^{\iota(e)-1}$.
\item $e' \not\in B^{\iota(e)-1} \implies e' \not\in\tilde{B}_e \backslash e$.
\end{enumerate}
\end{proof}

We can compute
\begin{align*}
  \Delta_+^{\min}(e) &= F(\tilde{A}_e) - F(\tilde{A}_e \backslash e),
& \Delta_+^{\max}(e) &= F(\hat{A}_e \cup e) - F(\hat{A})\\
  \Delta_-^{\min}(e) &= F(\tilde{B}_e) - F(\tilde{B}_e \cup e),
& \Delta_-^{\max}(e) &= F(\hat{B}_e \backslash e) - F(\hat{B}).
\end{align*}

\begin{cor} By submodularity of $F$, $\Delta_+^{\min}(e) \leq \Delta_+(e) \leq \Delta_+^{\max}(e)$, and $\Delta_-^{\min}(e) \leq \Delta_-(e) \leq \Delta_-^{\max}(e)$.
\end{cor}

\subsection{Separable sums $F$}
We maintain $\tilde\alpha_l$, $\hat\alpha_l$, $\tilde\beta_l$, $\hat\beta_l$.
It can be shown that $\hat\alpha_{l,e} \leq \alpha^{\iota(e)-1} \leq \tilde\alpha_{l,e} - w_l(e)$ and $\hat\alpha_{l,e} \geq \beta^{\iota(e)-1} \geq \tilde\beta_{l,e} + w_l(e)$, which then allows us to compute our bounds for $\Delta$'s.

~











\section{Analysis of algorithms}

\subsection{Approximation of hogwild bidirectional greedy}
\begin{thm}\label{thm:randomapprox} Let $F$ be a non-negative (monotone or non-monotone) submodular function.
The hogwild bidirectional greedy algorithm solves the unconstrained problem $\max_{A\subset V} F(A)$ with approximation
\[
E[F(A)] \geq \frac{1}{2}F^* - \frac{1}{4}\sum_{i=1}^n E[\rho_i],
\]
where $A$ is the output of the algorithm, $F^*$ is the optimal value, and $\rho_i$ is a random variable such that $\rho_i \geq \Delta_+^{\max}(i) - \Delta_+(i)$ and $\rho_i \geq \Delta_-^{\max}(i) - \Delta_-(i)$.
\end{thm}

We prove the theorem in Appendix \ref{app:proofhogwild}.

\textbf{Assumption}

$F$ is submodular and non-negative.
We assume that we can bound
\begin{align*}
&\Delta_+^{\max} - \rho_i \leq \Delta_+ \leq \Delta_+^{\max} \leq \Delta_+ + \rho_i,&
&\Delta_-^{\max} - \rho_i \leq \Delta_- \leq \Delta_-^{\max} \leq \Delta_- + \rho_i
\end{align*}

This is possible, for example, by defining
\begin{align*}
\rho_i
&:= \max\{\Delta_+^{\max}(i) - \Delta_+(i), \Delta_-^{\max}(i) - \Delta_-(i)\}\\
&\leq \max_{S,T\subseteq V} \{[F(S\cup i) - F(S)] - [F(S \cup T \cup i) - F(S \cup T)]\}\\
&\leq F(i) - F(\emptyset) - F(V) + F(V\backslash i)\\
&\leq F(i)\left(1 - \frac{F(V) - F(V\backslash i)}{F(i)}\right)\\
&= F(i)\kappa_F
\end{align*}
where $S$ plays the role of $A^j$ and $T$ plays the role of $\{j+1,\dots, i-1\}$, and $\kappa_F$ is the total curvature of $F$.
Summing over $i$ then gives us $\sum_i \rho_i \leq \kappa_F\sum_i F(i)$.

\taunghao{Is there theory along these lines? Can we tighten this for non-monotone functions?}


\textbf{Example: max graph cut}

Assuming bounded delay of $\tau$ and edges with unit weight, we can bound $\sum_i E[\rho_i] \leq 2\tau\frac{\text{\#edges}}{2N}$
The approximation of the hogwild algorithm is then $E[F(A^n)] \geq = \frac{1}{2} F(OPT) - \tau\frac{\#\text{edges}}{2N}$.
In sparse graphs, the hogwild algorithm is off by a small additional term, which albeit grows linearly in $\tau$.



\textbf{Example: set cover}

\taunghao{For now, consider a toy problem, with (1) disjoint sets, (2) bounded delay, (3) $\lambda \leq 1$.}

Consider the simple set cover function,
$F(A) = \sum_{l=1}^L \min(1,|A\cap S_l|) - \lambda|A| = |\{l: A\cap S_l \neq\emptyset\}| - \lambda|A|$,
with $0 < \lambda \leq 1$.
We assume that there is some bounded delay $\tau$.
Suppose also the $S_l$'s form a partition, so each element $e$ belongs to exactly one set.
Then, $\sum_e E[\rho_e] \geq \tau + L(1-\lambda^\tau)$, which is linear in $\tau$ but independent of $N$.


\subsection{Correctness of OCC}
\begin{thm} OCC bidirectional greedy is serially equivalent to bidirectional greedy.
\end{thm}
\begin{proof}
Outline of proof: We need to show 2 things.
Firstly, that the sampling using $\Delta_+^{min}$, $\Delta_+^{max}$, $\Delta_-^{min}$, $\Delta_-^{max}$ is `safe', i.e. is equivalently to sampling using $\Delta_+$ and $\Delta_-$.
Secondly, that the validation process is correct -- specifically that when the validation is executed, it is in fact the case that $\hat{A} = A^{\iota(e)-1}$ and $\hat{B} = B^{\iota(e)-1}$.
\end{proof}



\subsection{Scalability of OCC}

We discuss the bound on the number of elements sent for validation in Appendix \ref{app:proofocc}

\textbf{Example: max graph cut}

The expected number of validated elements is upper bounded by $\tau \frac{2\#edges}{N}$.

\textbf{Example: set cover}

Under the same settings as for the hogwild analysis, the expected number of validated elements is upper bounded by $2\tau$.

















\section{Evaluation}

\subsection{Implementation}
Code in Java / Scala.

\subsection{Experiments}
Experiments run on laptop.

\textbf{Set Cover:} 5000 elements, covering 500,000 groups, in a random graph with edge probability of 0.002

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\# threads} & \multicolumn{3}{c|}{OCC} & \multicolumn{4}{c|}{Hogwild}\\
 & Runtime & Relative & \# validated & Runtime & Relative & $F(A)$ & \# diff from seq.\\\hline
Sequential &	 405 &	 2.55 &	 0 &	 328 &	 1.67 &	 496516 &	 0\\\hline
1&	 1032&	 1&	 0&	 549&	 1&	 496516&	 0\\\hline
2&	 504&	 2.05&	 0&	 341&	 1.61&	 496516&	 1\\\hline
3&	 367&	 2.81&	 2&	 171&	 3.21&	 496515&	 4\\\hline
4&	 327&	 3.16&	 7&	 149&	 3.68&	 496516&	 5\\\hline
5&	 299&	 3.45&	 6&	 129&	 4.26&	 496516&	 6\\\hline
\end{tabular}

\textbf{Graph Cut:} 20,000 vertices, in a random graph with edge probability of 0.2

\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
\multirow{2}{*}{\# threads} & \multicolumn{3}{c|}{OCC} & \multicolumn{4}{c|}{Hogwild}\\
 & Runtime & Relative & \# validated & Runtime & Relative & $F(A)$ & \# diff from seq.\\\hline
Sequential &	 1142 &	 1.21 &	 0 &	 1482 &	 0.90 &	 40047848 &	 0\\\hline
1&	 1381&	 1&	 0&	 1341&	 1&	 40047848&	 0\\\hline
2&	 666&	 2.07&	 2&	 649&	 2.07&	 40047682&	 24\\\hline
3&	 461&	 3.00&	 7&	 465&	 2.88&	 40047702&	 22\\\hline
4&	 448&	 3.08&	 14&	 385&	 3.48&	 40047920&	 5\\\hline
5&	 379&	 3.64&	 19&	 337&	 3.98&	 40047664&	 24\\\hline
\end{tabular}

Runtime is given in milliseconds.
`Relative' runtime refers to the runtime relative to that of using 1 thread.
Ideally, the runtime of the parallel algorithm should be lesser than the sequential algorithm, and have a relative runtime that scales linearly with the number of threads (e.g. running on 5 threads should have a relative runtime of 5).

Due to the effort needed for coordination and maintaining additional sets, the OCC algorithm is initially slower than the sequential algorithm, but quickly outperforms it as more threads are added.
We counted the number of elements that had to be validated, which required major coordination and slow down in the algorithm.
The number of validated elements increased with the number of threads (which explains the failure of OCC to scale less than linearly), but is still less than 1\% of the total number of elements when 5 threads are used.

We also verified the correctness of the OCC algorithm by comparing the output sets with the sequential algorithm executed under the same ordering of elements.

The hogwild algorithm has better scaling than OCC, since it requires almost no coordination.
We also measured the objective value $F(A)$ for hogwild.
For the set cover problem, hogwild performs just as well as the sequential algorithm, even though it does not provide exactly the same output set.
On the graph cut problem, the difference in the objective value is less than 0.01\%.



{\footnotesize
%\subsection*{Acknowledgments}
%This research is supported in part by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, and  gifts from Amazon Web Services, Google, SAP,  Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!.
%This material is also based upon work supported in part by the Office of
%Naval Research under contract/grant number N00014-11-1-0688. 
%X. Pan's work is also supported in part by a DSO National Laboratories Postgraduate Scholarship.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliographystyle{unsrtnat}
\bibliography{references_arxiv}
}

\newpage
\appendix

\input{hogwildproof}

\newpage\input{occproof}


\newpage\section{Lemma}
\begin{lem}\label{lem:sumbinomial} $\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j} = {a+1 \choose b+1}$.
\end{lem}
\begin{proof}
\begin{align*}
&\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j}\\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose t-j} {a-k'-t+j \choose b-t+j} \\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose k'} {a-k'-t+j \choose a-b-k'} & \text{(symmetry of binomial coeff.)}\\
&= (-1)^{a-b}\sum_{k'=0}^{a-b} {-t+j-1 \choose k'} {-b+t-j-1 \choose a-b-k'} & \text{(upper negation)}\\
&= (-1)^{a-b} {-b-2 \choose a-b} & \text{(Chu-Vandermonde's identity)}\\
&= {a+1 \choose a-b} & \text{(upper negation)}\\
&= {a+1 \choose b+1} & \text{(symmetry of binomial coeff.)}\\
\end{align*}
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{comment}
\section{Discussion on $\rho_i$}

\section{Hogwild approximation for monotone $F$}
\subsection{Proof for monotone functions}
\begin{lem}\label{lem:singleelement_monotone} If $F$ is monotone submodular, for every $1 \leq i \leq n$,
\[E[F(O^{i-1})-F(O^i)] \leq \frac{1}{2(1-\kappa_F)} E[f(A^i) - f(A^{i-1}) + f(B^i) - f(B^{i-1})].\]
\end{lem}
\begin{proof}
Since $F$ is monotone, we only have to check the case where $0 \leq \Delta_+(i) \leq \Delta_+^{\max}(i)$ and $0 \leq \Delta_-(i) \leq \Delta_-^{\max}(i)$.
Following the proof structure of Case 8 in Lemma \ref{lem:singleelement}, we get
\begin{align*}
E[f(A^i) - f(A^{i-1}) | A^{i-1}, j] &\geq \frac{(1+\kappa_F)\Delta_+^{\max}(i)^2}{\Delta_+^{\max}(i) + \Delta_-^{\max}(i)}\\
E[f(B^i) - f(B^{i-1}) | A^{i-1}, j] &\geq \frac{(1+\kappa_F)\Delta_-^{\max}(i)^2}{\Delta_+^{\max}(i) + \Delta_-^{\max}(i)}\\
E(f(O^{i-1}) - f(O^i) | A^{i-1}, j] &\leq \frac{\Delta_+^{\max}(i)\Delta_-^{\max}(i)}{\Delta_+^{\max}(i) + \Delta_-^{\max}(i)}
\end{align*}
\begin{align*}
&E[F(O^{i-1}) - F(O^i) | A^{i-1}, j] - \frac{1}{2(1-\kappa_F)} E[f(A^i) - f(A^{i-1}) + f(B^i) - f(B^{i-1})| A^{i-1}, j]\\
&\leq \frac{1/2}{\Delta_+^{\max}(i) + \Delta_-^{\max}(i)}\bigg[
2\Delta_+^{\max}(i)\Delta_-^{\max}(i)
- \Delta_-^{\max}(i)^2
- \Delta_+^{\max}(i)^2
\bigg]\\
&= \frac{1/2}{\Delta_+^{\max}(i) + \Delta_-^{\max}(i)}\bigg[-(\Delta_+^{\max}(i) - \Delta_-^{\max}(i))^2\bigg]\\
&\leq 0.
\end{align*}
\end{proof}

\begin{thm}\label{thm:randomapprox_monotone} Let $F$ be a non-negative monotone submodular function.
The hogwild bidirectional greedy algorithm solves the unconstrained problem $\max_{A\subset V} F(A)$ with approximation
\[
E[F(A)] \geq \frac{1-\kappa_F}{2-\kappa_F}F^*,
\]
where $A$ is the algorithm output, $F^*$ is the optimal value, and $\kappa_F$ is the total curvature of the function.
\end{thm}
\begin{proof}
Summing up the statement of Lemma \ref{lem:singleelement_monotone} for all $i$ gives us a telescoping sum, which reduces to:
\begin{align*}
E[F(O^0)-F(O^n)]
&\leq \frac{1}{2(1-\kappa_F)} E[F(A^n) - F(A^0) + F(B^n) - F(B^0)]\\
&\leq \frac{1}{2(1-\kappa_F)} E[F(A^n) + F(B^n)].
\end{align*}
Note that $O^0 = OPT$ and $O^n = A^n = B^n$, so $E[F(A^n)] \geq \frac{1-\kappa_F}{2-\kappa} F(OPT)$.
\end{proof}
\end{comment}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
