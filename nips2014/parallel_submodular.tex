
\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}


% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{caption}
\usepackage{subcaption}
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{listings}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{float}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{wrapfig}
\setlength{\emergencystretch}{3em}
\usepackage[numbers]{natbib}

\usepackage{thm-restate}


\usepackage{multirow}

% For algorithms
\usepackage[algoruled,vlined,linesnumbered]{algorithm2e}
%\usepackage{algorithm}
%\usepackage{algorithmic}

\usepackage{multicol}
\usepackage{comment}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2013} with
% \usepackage[nohyperref]{icml2013} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}


\graphicspath{{figures/}}


\newcommand{\ie}{{\em i.e.,}~}
\newcommand{\eg}{{\em e.g.,}~}

\newcommand{\hogwild}{CF2G}
\newcommand{\occ}{CC2G}
\newcommand{\seqalg}{Seq2G}

\begingroup
    \makeatletter
    \@for\theoremstyle:=definition,remark,plain\do{%
        \expandafter\g@addto@macro\csname th@\theoremstyle\endcsname{%
            \addtolength\thm@preskip\parskip
            }%
        }
\endgroup
\newtheorem{dfn}{Definition}[section]
\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{exmp}[thm]{Example}
\newtheorem{claim}{Claim}

\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Program}

\newenvironment{denseitemize}{
\begin{itemize}[topsep=2pt, partopsep=0pt, leftmargin=1.5em]
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{itemize}}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{4pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}




\lstdefinelanguage{scala}{
  morekeywords={abstract,case,catch,class,def,%
    do,else,extends,false,final,finally,%
    for,if,implicit,import,match,mixin,%
    new,null,object,override,package,%
    private,protected,requires,return,sealed,%
    super,this,throw,trait,true,try,%
    type,val,var,while,with,yield},
  otherkeywords={=>,<-,<\%,<:,>:,\#,@},
  sensitive=true,
  morecomment=[l]{//},
  morecomment=[n]{/*}{*/},
  morestring=[b]",
  morestring=[b]',
%  numbers=left,
  morestring=[b]"""
}



% Commenting system
\newcommand{\Comments}{1}
\newcommand{\note}[2]{\ifnum\Comments=1\textcolor{#1}{#2}\fi}
\newcommand{\xinghao}[1]{\note{red}{[XP: #1]}}
\newcommand{\joey}[1]{\note{blue}{[JG: #1]}}
\newcommand{\stef}[1]{\note{green}{[SJ: #1]}}
\newcommand{\joseph}[1]{\note{cyan}{[JB: #1]}}

\newcommand{\argmin}{\operatornamewithlimits{argmin}}






%% ---------------------------------------------------------
%% Terminology
\newcommand{\term}[1]{\textbf{#1}}


%% ---------------------------------------------------------
%% Citation/Reference commands
\newcommand{\citecf}[1]{(\cf, \cite{#1})}
\newcommand{\tableref}[1]{Table~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\listref}[1]{Listing~\ref{#1}}

\newcommand{\eqnref}[1]{Eq.~(\ref{#1})}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}

\newcommand{\dfnref}[1]{Definition~\ref{#1}}
\newcommand{\thmref}[1]{Theorem~\ref{#1}}
\newcommand{\propref}[1]{Prop.~\ref{#1}}
\newcommand{\lemref}[1]{Lemma~\ref{#1}}
\newcommand{\exmpref}[1]{Example~\ref{#1}}
\newcommand{\corref}[1]{Corollary~\ref{#1}}
\newcommand{\algref}[1]{Alg.~\ref{#1}}
\newcommand{\procref}[1]{Proc.~\ref{#1}}
\newcommand{\alglineref}[1]{Line~\ref{#1}}
\newcommand{\probref}[1]{Problem~(\ref{#1})}
\newcommand{\appendref}[1]{Appendix~\ref{#1}}

%% ---------------------------------------------------------
%% Basic Math
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}


%% ---------------------------------------------------------
%% special math functions
\newcommand{\polylog}[2]{\,\mathbf{Li}_{#1}\left( #2 \right)}
\newcommand{\harmonic}[2]{\,\mathbf{h}_{#1}\left( #2 \right)}

%% ---------------------------------------------------------
%% Norms
\newcommand{\Lone}{L_{1}}
\newcommand{\Linf}{L_{\infty}}
\newcommand{\LInfNorm}[1]{\left|\left| #1 \right|\right|_{\infty}}
\newcommand{\LOneNorm}[1]{\left|\left| #1 \right|\right|_1}

\newcommand{\hinge}[1]{\left[  #1 \right]_+}

%% ---------------------------------------------------------
%% Probability notation
\newcommand{\given}{\,|\,}
\newcommand{\stdist}[1]{\mathbf{\pi} \left( #1 \right) }
\newcommand{\Prb}[1]{\mathbf{P} \left( #1 \right) }
\newcommand{\PrbEst}[1]{\mathbf{\tilde{P}} \left( #1 \right) }
\newcommand{\Ent}[1]{\mathbf{H} \left( #1 \right) }
\newcommand{\PiPrb}[1]{\Prb{ #1 } }
\newcommand{\Kern}[1]{K \left( #1 \right) }
\newcommand{\Ex}[1]{\mathbf{E} \left[ #1 \right] }
\newcommand{\Exwrt}[2]{\mathbf{E}_{#1} \left[ #2 \right] }
\newcommand{\Variance}[1]{\mathbf{Var} \left[ #1 \right] }
\newcommand{\Ind}[1]{\mathbf{1}\left[ #1 \right]}
\newcommand{\Bern}[1]{\text{Bern}( #1 ) }

%% ---------------------------------------------------------
%% Set notation
\newcommand{\reals}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\vecspace}{\mathcal{V}}
\newcommand{\Union}{\bigcup}
\newcommand{\Inter}{\bigcap}
\newcommand{\union}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\size}[1]{\left| #1 \right|}


%% ---------------------------------------------------------
%% Complexity
\newcommand{\BigO}[1]{O\hspace{-1pt}\left( #1 \right)}
\newcommand{\BigTheta}[1]{\Theta \left( #1 \right)}
\newcommand{\BigOmega}[1]{\Omega \left( #1 \right)}





%% ---------------------------------------------------------
%% Algorithms
\SetKwFor{ParForAll}{for}{do in parallel}{end}
\SetKwFunction{Map}{Map}
\SetKwFunction{Reduce}{Reduce}

\SetKwInput{Input}{Input}
\SetKwInput{Output}{Output}
\SetKwInput{SideEffect}{SideEffect}
\SetKwInput{Define}{Define}
\SetKwInput{Global}{Global}
\SetKwFor{DoWithProbability}{with probability}{}{}
\SetKwFunction{DPMeansOp}{DPMeansOp}
\SetKwFunction{DPValidate}{DPValidate}
\SetKwFunction{OFLValidate}{OFLValidate}
\SetKwFunction{BPMeansOp}{BPMeansOp}
\SetKwFunction{BPValidate}{BPValidate}
\SetKwFunction{NewClusters}{AcceptedClusters}

\SetKw{WaitUntil}{wait until}

\SetKwFunction{Mean}{Mean}
\SetKwFunction{Ref}{Ref}


%% ---------------------------------------------------------
%% Paper specific notation

% All the data
\newcommand{\data}{\mathcal{D}}
% \datablock{machine}
\newcommand{\datablock}[1]{\data_{#1}}

\newcommand{\clusters}{\mathcal{C}}
\newcommand{\gclusters}{\hat\clusters}
\newcommand{\newclusters}{\tilde\clusters}
% local clusters \lclusters{machine}
\newcommand{\lclusters}[1]{\clusters_{#1}}

%\newcommand{\bregd}[2]{D_\phi\left(#1,#2\right)}
\newcommand{\bregd}[2]{\left\|#1-#2\right\|}


% for ofl analysis:
\newcommand{\CFL}{C^{\text{FL}}}
\newcommand{\muFL}{\mu^{\text{FL}}}


\title{Parallel Double Greedy Submodular Maximization}

\author{
Xinghao Pan$^1$ Joseph Gonzalez$^1$ Stefanie Jegelka$^1$ Joseph Bradley$^{1}$ Michael I. Jordan$^{1,2}$\\
$^1$Department of Electrical Engineering and Computer Science, and $^2$Department of Statistics\\
University of California, Berkeley\\
Berkeley, CA USA 94720\\
  \texttt{\{xinghao,jegonzal,stefje,tab,?\}@eecs.berkeley.edu} \\
}

% \address{University of California
% 465 Soda Hall, MC-1776
% Berkeley, CA 94720-1776}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


%\nipsfinalcopy

\begin{document}


\maketitle


\begin{abstract}
Many machine learning problems can be reduced to the maximization of a submodular function. While such a formulation can gain theoretical guarantees, exploiting these results in a parallel setting remains an open research topic. In particular, recently, \citet{buchbinder2012} achieved a tight 1/2-approximation for unconstrained submodular maximization using a double greedy algorithm. This algorithm was developed and analyzed in the serial setting, limiting our ability to leverage parallel hardware. In this work, we bridge the gap between those theoretical benefits and scalable computing. We propose and analyze two parallel extensions to the double greedy algorithm.
The first, \emph{coordination-free} approach emphasizes speed at the cost of a weaker approximation guarantee.
The second, \emph{concurrency control} approach guarantees the tight 1/2-approximation, at the potential, quantifiable cost of additional coordination and reduced parallelism.
% We bound both the weaker approximation factor and the reduction in parallelism.
We implement and evaluate both algorithms on multi-core hardware and billion edge graphs demonstrating both the scalability and tradeoffs of each approach.
%
%Many machine learning problems can be reduced to the maximization of a submodular function. 
%Recently, \citet{buchbinder2012} achieved a tight 1/2-approximation for unconstrained submodular maximization using a double greedy algorithm.
%Unfortunately, the double greedy algorithm was developed and analyzed in the serial setting limiting our ability to leverage parallel hardware.
%In this work we propose and analyze two parallel extensions to the \cite{buchbinder2012} double greedy algorithm.
%The first, \emph{coordination-free} approach emphasizes speed at the cost of a weaker approximation guarantee.
%The second, \emph{concurrency control} approach guarantees the same tight 1/2-approximation, at the cost of additional coordination and reduced parallelism.
%We bound both the weaker approximation factor and the reduction in parallelism.
%We implement and evaluate both algorithms on multi-core hardware and billion edge graphs demonstrating both the scalability and tradeoffs of each approach.
%
% Many machine learning problems can be formulated as maximization of submodular functions. \cite{buchbinder2012} recently proposed a double greedy algorithm for unconstrained submodular maximization that achieves a tight 1/2-approximation. However, double greedy is an inherently sequential, linear time algorithm that does not scale to big data.

% We present two approaches to extend the bidirectional greedy algorithm to a parallel setting.
% The first, `coordination-free' approach emphasizes speed at the cost of a weaker approximation guarantee -- it achieves a $(1/2 - O(1/N))$ approximation for max cut on a complete graph.
% The second, `concurrency control' approach guarantees the same tight 1/2-approximation, at the cost of greater coordination.

% Our parallel algorithms scale well on synthetic and real datasets, and suffer little or no loss of objective value compared to the sequential algorithm.
\end{abstract}

\section{Introduction}


Motivate parallel machine learning from big data setting

Briefly mention two approaches (coordination-free, concurrency control)

In this paper, we focus on unconstrained non-monotone submodular maximization.. and say briefly why this problem is important

Contributions of the paper
\begin{enumerate}
\item We propose two parallel algorithms for unconstrained non-monotone submodular maximization, which allows one to choose between speed and stronger approximation guarantees.
\item We prove the serial equivalence of \occ{} to the sequential algorithm; we analytically bound the amount of coordination of the \occ{} algorithm for two example problems.
\item We provide approximation guarantees for \hogwild{}; we analytically bound the expected loss in objective value for two example problems.
\item We demonstrate empirically using two synthetic and three real datasets that our parallel algorithms perform well in terms of both speed and objective values.
\end{enumerate}



The bidirectional greedy algorithm \cite{buchbinder2012} gives an approximation of $E[F(A)] = 1/2 f(OPT)$, where $A$ is the algorithm output, and $OPT$ is an optimal solution.

The \hogwild{} algorithm can give an approximation of $E[F(A)] = \frac{1}{2} F(OPT) - \frac{1}{4}\sum_iE[\rho_i]$, where $\rho_i$ is the maximum difference in the marginal gain that may result from not knowing the full information when deciding whether to include or exclude element $i$.

The \occ{} algorithm \xinghao{for the lack of a better name} guarantees an outcome that is equivalent to a sequential run of the bidirectional greedy algorithm.
Theoretical properties of the bidirectional greedy algorithm immediately translates to the \occ{} algorithm -- in particular, the \occ{} algorithm gives the same approximation factor of $1/2$.
In contrast to the \hogwild{} approach, \occ{} introduces more coordination and thus provides less concurrency.











\section{Submodular maximization}
A set function $F: 2^V \to \mathbb{R}$ defined over subsets of a ground set $V$ is \emph{submodular} if it satisfies \emph{diminishing marginal returns}: for all $A \subseteq B \subseteq V$ and  $e \notin V$, it holds that $F(A \union \{e\}) - F(A) \geq F(B \union \{e\}) - F(B)$. Throughout this paper, we will assume that $F$ is nonnegative and $F(\emptyset)=0$. Submodular functions have emerged in areas such as game theory \cite{}, combinatorial optimization \cite{}, stochastic processes \cite{}, and, more recently, in machine learning too \cite{tutorial,tutorial2}. Problems such as sensor placement \cite{krauseGuestrin11}, image co-segmentation \cite{kim11}, MAP inference with determinantal point process priors \cite{}, influence maximization \cite{} or document summarization \cite{} may be phrased as the maximization of a submodular function. Such a formulation allows to use algorithms for submodular maximization \cite{} that offer theoretical worst-case guarantees on the quality of the solution.

While those algorithms are theoretically beneficial and tend to work very well in practice, their design is inherently sequential. But increasingly often, one wishes to solve the above-mentioned problems at very large scale. This development raises the question of parallel algorithms for submodular maximization that ideally preserve the theoretical bounds, or weaken them gracefully, in a quantifiable manner. 

Recent work has addressed parallel versions of the greedy algorithm by \cite{} for maximizing \emph{monotone} submodular functions that satisfy $F(A) \leq F(B)$ for any $A \subseteq B \subseteq V$ \cite{krause, mapgreedy}. However, in several important cases such as MAP inference, zoubin, and when trading off gains with (linear) cost functions (functions of the form $F(S) = G(S) + \lambda M(S)$, where $G(S)$ is monotone submodular and $M(S)$ is a linear (modular) cost function), we aim to maximize a \emph{non-monotone} submodular function to which the greedy algorithm of \cite{} no longer applies. For the non-monotone case, \citet{buchbinder2012} recently proposed an optimal double greedy algorithm that they develop and analyze in a serial setting. Here, we theoretically and empirically study parallel analogs of this algorithm.

\paragraph{The sequential double greedy algorithm.}
The sequential double greedy algorithm (Algorithm \ref{alg:submax}) maintains two sets $A^i \subseteq B^i$. Initially, $A^0 = \emptyset$ and $B^0 = V$. In iteration $i$, the set $A^i$ contains the items selected before item/iteration $i$, and $B^i$ contains $A^i$ and the items that are so far undecided. The algorithm sequentially passes through the items in $V$ and determines online whether to keep item $i$ (add to $A^i$) or discard it (remove from $B^i$). The decision depends on the gain of $i$ with respect to $A^i$, and the gain of removing $i$ from $B^i$.

%Describe the bidirectional greedy algorithm:
%The sequential bidirectional greedy \cite{buchbinder2012} algorithm monotonically grows $A^i$ and shrinks $B^i$.









\section{Concurrency Patterns for Parallel Machine Learning}

In this paper we adopt a transactional view of the program state and explore parallelization strategies through the lens of parallel transaction processing systems.
We recast the program state, the sets $A$ and $B$, as data, and the operations, adding elements to $A$ and removing elements from $B$, as transactions.
More precisely we reformulate the bidirectional greedy algorithm (\algref{alg:submax}) as a series of \emph{exchangeable}, \emph{Read-Write} transactions of the form:
\begin{equation}
T_e(A,B) :=
\begin{cases}
   (A \union e, B) & \text{if } u_e \leq \frac{\hinge{\Delta_+(A,e)}}{ \hinge{\Delta_+(A,e)} + \hinge{\Delta_-(B,e)}}  \\
   (A, B \backslash e) & \text{otherwise. }
  \end{cases}
  \label{eqn:greedytransaction}
\end{equation}
The transaction $T_e$ is a function from the sets $A$ and $B$ to new sets $A$ and $B$ based on the element $e \in V$ and the predetermined random bits $u_e$ for that element.
We fixed the source of randomness in advance to simplify the presentation as $T_e$ becomes a deterministic function.


By composing the transactions $T_n (T_{n-1}(\ldots T_1(\emptyset, V)))$ we recover the serial double-greedy algorithm defined in \algref{alg:submax}.
In fact, any ordering of the \emph{serial} composition of the transactions recovers a permuted execution of \algref{alg:submax} and therefore the optimal approximation algorithm.
However, this raises the question: \emph{is it possible to apply transactions in parallel?}
If we execute transactions $T_i$ and $T_j$, with $i \neq j$, in parallel we
need a method to merge the resulting program states.
In the context of the double greedy algorithm, we could define the parallel execution of two transactions as:
\begin{equation}
T_i(A,B) + T_j(A,B) = \left(T_i(A,B)_A \union T_j(A,B)_A,  \,\, T_i(A,B)_B  \inter T_j(A,B)_B \right).
\label{eqn:merge}
\end{equation}
the union of the resulting $A$ and the intersection of the resulting $B$.
While we can easily generalize \eqnref{eqn:merge} to many parallel transactions, we cannot always guarantee that the result will corresponds to a serial composition of transactions.
As a consequence, we cannot directly apply the analysis of Buchbinder et al.~\cite{buchbinder2012} to derive strong approximation guarantees for our parallel execution.

Fortunately, several decades of research~\citep{Ozsu07,kung1981:occ} in database systems have explored the design of efficient parallel transaction processing systems.
The systems span a space ranging from extremely fast coordination free approaches which provide minimal guarantees on the composition of transactions to sophisticated concurrency control techniques which ensure a direct correspondence between parallel and serial executions at the cost of slightly reduced performance.




% A common technique is to introduce a validation stage that detects inconsistent transactions and prescribes a compensating action.
% In \algref{alg:generalparallel} we describe a general validation based meta-algorithm that we extend in subsequent sections.
% Each processor extracts a next element to process and a consistent but potentially out of data snapshot of the program state.
% Based on the snapshop and selected element, each processor proposes a transaction, $\partial_e$, and the preconditions under which the transaction is valid, $\mathfrak{A}$.
% We denote the transaction as $\partial_e$ to avoid confusion with \eqnref{eqn:greedytransaction} and because it encodes the effect (\eg remove $e$ from $B$) of more costly transactions (\eg \eqnref{eqn:greedytransaction}).


% After proposing a transaction and corresponding preconditions, the processor validates the transaction by invoking the validation process defined in \algref{alg:generalparallel:validate}.
% In this work we restrict our attention to an atomic validation process.
% The validation function ensures that the proposed transaction hasn't already failed and that the preconditions are satisfied.
% If the transaction is invalid, then a compensating action is taken.
% Here we consider simple compensating actions in which a valid transaction is constructed.
% Finally, the valid transaction is applied advancing the program state.


% By changing the preconditions and proposal process we can span a range of potential parallelization strategies.
% By weakening the preconditions we can minimized or even eliminate invalid transactions and the need for validation.
% Alternatively, by carefully constructing the preconditions we can guarantee a serializable execution potentially at the expense of parallel scalability.

\xinghao{Casting our meta-algorithm as a generalization of (pessimiistic) concurrency approach.}
In \algref{alg:generalparallel} we describe a common pattern for concurrency control.
A transaction begins by requesting a set of guarantees $\mathfrak{G}_e$ and an associated logical time $i$, such that $\mathfrak{G}_e$ is guaranteed to hold at $i$.
Under these conditions, the transaction attempts to propose and commit an update.
However, if the guaranteed conditions are insufficient to do so, the transaction is rejected and the update is recomputed by the server.
\xinghao{We could add a footnote here that states that an alternative approach which we do not discuss in detail is for the transaction to rollback and re-try.}
The server is in charge of providing and guaranteeing the conditions, and processing commits in the logical order of $i$.
It is straightforward to see that the output of the parallel execution is $\partial_n (\partial_{n-1}(\ldots \partial_1(\emptyset, V)))$.
A parallel algorithm is said to be \textit{serially equivalent} to a sequential algorithm if for any input, the output of the parallel algorithm corresponds to that of some sequential execution.

\xinghao{\occ:} In our \occ{} double greedy algorithm, these conditions correspond to upper and lower bounds on the sets $A$ and $B$ at commit time.


\xinghao{\hogwild:} The coordination-free approach attempts to reduce the need to coordinate guarantees and logical ordering.
This is achieved by operating on potentially stale states -- the guarantee reduces to requiring $\mathfrak{G}_e$ be a stale version of $\mathfrak{S}$, and logical ordering is implicitly defined by the time of commit.
\xinghao{An interesting take on hogwild -- hogwild is not serially equivalent to the sequential algorithm, but it is serially equivalent to an approximate algorithm that operates on stale states.
Thus, to understand hogwild, we analyze the approximate algorithm.
The concept of CC buys us two things: firstly, to extend sequential algorithms into parallel ones; secondly, to project a parallel algorithm into a sequential algorithm, which aids our understanding and analysis.}





%\Joe scratch space.

%  correspond to preconditions

% necessary to maintain an invariant based notion of validity which departs fro more classic validation models.

%  conditional notion validity is a departure from more classic validation models and more closely resembles the notion of

% Finally, a validation function is called which verifies that the conditions $\mathfrak{S}$ are still valid with the current global state and if so atomically applies the operation

% Inspired by the work of Xinghao et al.~\cite{Xinghao13} we adopt a

% In this paper we propose and compare two techniques.  The simplest, inspired by Recht et al.~\cite{Recht11} is a coordination free approach which provides weak guarantees.

% towards coordination free approaches

% several techniques from this literature and propose both a coordination free approach as well as an approach based on optimistic concurrency control
% to parallel transaction processing with an optimistic concurrency control approach inspired by the recent work of


% \subsection{Coordination free}
% The coord

% \joey{finish}
% Simply run everything in parallel.
% Optimized for speed, but does not necessarily provide the correct answer.
% Requires work to prove correctness.

% \subsection{Concurrency control}
% Ensures `serial equivalence' -- the outcome of the parallel algorithm is equivalent to some execution of the sequential algorithm.
% Locally, threads take actions that are guaranteed to be safe (i.e. preserves serial equivalence), and forces additional coordination only when they are unable to execute their action safely.
% Designed for correctness, but requires coordination that compromises speed.
% Work is only required to demonstrate that coordination is limited.


\begin{figure}[h]
  \footnotesize
  \centering
  \begin{multicols}{2}
    \begin{minipage}{0.45\textwidth}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Generalized transactions}
        \label{alg:generalparallel}
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $(\mathfrak{G}_e, i) = \text{requestGuarantee}(e)$\;
            $\partial_i$ = propose($e$, $\mathfrak{G}_e$)\;
            commit($e$, $i$, $\partial_i$) \tcp{Non-blocking}
          }
        }
      \end{algorithm}

    \end{minipage}

    \begin{minipage}{0.45\textwidth}
      \begin{algorithm}[H]
        \SetKwBlock{Atomically}{Atomically}{}
        \DontPrintSemicolon
        \caption{Commit}
        \label{alg:generalparallel:commit}
        \WaitUntil $\forall j < i$, processed$(j) = true$\;
        \Atomically{
        \If{$\partial_i =\text{Fail}$}{
          \tcp{Reject and compensate}
          $\partial_i$ = propose($e$, $\mathfrak{S}$)\;
        }
        \tcp{Advance program state}
        $\mathfrak{S} \leftarrow \partial_i(\mathfrak{S})$\;
        }
      \end{algorithm}
    \end{minipage}



  \end{multicols}
  \caption{\footnotesize Algorithm for generalized transactions. Each transaction requests for its position $i$ in the commit ordering, as well as the bounds $\mathfrak{G}_e$ that are guaranteed to hold when it commits. Transactions are also guaranteed to be committed according to the given ordering.}
  \label{fig:submax}
\end{figure}





\begin{figure}[h]
  \footnotesize
  \centering
  \begin{multicols}{2}
    \begin{minipage}{0.49\textwidth}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{Serial submodular maximization}
        \label{alg:submax}
        %\Input{}
        $A^0 = \emptyset$, $B^0 = V$\;
        \For{$i = 1$ to $n$}{
          $\Delta_{+}(i) = F(A^{i-1}\cup i) - F(A^{i-1})$\;
          $\Delta_{-}(i) = F(B^{i-1}\backslash i) - F(B^{i-1})$\;
          Draw $u_i\sim Unif(0,1)$\;
          \If {$u_i<\frac{\hinge{\Delta_{+}(i)}}{\hinge{\Delta_{+}(i)} + \hinge{\Delta_{-}(i)}}$}{
            $A^i := A^{i-1} \cup i$;
            $B^i := B^{i-1}$\;
          }\lElse{
            $A^i := A^{i-1}$;
            $B^i := B^{i-1}\backslash i$
          }
        }
        %\Output{$A_n$}
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\hogwild{} bidirectional greedy}
        \label{alg:hogwild}
        \lFor{$e\in V$}{$\hat{A}(e) = 0$, $\hat{B}(e) = 1$}
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $\Delta_{+}^{\max}(e) = F(\hat{A}\cup e) - F(\hat{A})$\;\label{alg:hogwild:deltaadd}
            $\Delta_{-}^{\max}(e) = F(\hat{B}\backslash e) - F(\hat{B})$\;\label{alg:hogwild:deltarem}
            Draw $u_e\sim Unif(0,1)$\;\label{alg:hogwild:time}
            \If {$u_e<\frac{[\Delta_{+}^{\max}(e)]_+}{[\Delta_{+}^{\max}(e)]_+ + [\Delta_{-}^{\max}(e)]_+}$}{
              $\hat{A}(e) \leftarrow 1$\;\label{alg:hogwild:add}
            }\lElse{
              $\hat{B}(e) \leftarrow 0$\label{alg:hogwild:rem}
            }
          }
        }
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{} bidirectional greedy}
        \label{alg:occ}
        \lFor{$e\in V$}{$\hat{A}(e) = \tilde{A}(e) = 0$, $\hat{B}(e) = \tilde{B}(e) = 1$}
        % \lFor{$i = 1,\dots,|V|$}{result$(i) = 0$}
        \lFor{$i = 1,\dots,|V|$}{processed$(i) = false$}
        $\iota = 0$\;
        \ParForAll{$p \in \set{1, \ldots, P}$}{
          \While{$\exists$ element to process}{
            $e = $ next element to process\;
            $(\hat{A}_e, \tilde{A}_e, \hat{B}_e, \tilde{B}_e, i)$ = getSnapshot($e$)\;
            (result, $u_e$) = propose($e$, $\hat{A}_e$, $\tilde{A}_e$, $\hat{B}_e$, $\tilde{B}_e$)\;
            validate($e$, $i$, $u_e$, result)
          }
        }
      \end{algorithm}


    \end{minipage}

    \begin{minipage}{0.49\textwidth}
      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{} getSnapshot($e$)}
        \label{alg:occsnapshot}
        $\tilde{A}(e) \leftarrow 1$;
        $\tilde{B}(e) \leftarrow 0$\;
        $i = \iota$;
        $\iota \leftarrow \iota + 1$\;\label{alg:occ:time}
        $\hat{A}_e = \hat{A}$;
        $\hat{B}_e = \hat{B}$\;
        $\tilde{A}_e = \tilde{A}$;
        $\tilde{B}_e = \tilde{B}$\;
        \Return $(\hat{A}_e, \tilde{A}_e, \hat{B}_e, \tilde{B}_e, i)$
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{} propose}
        \label{alg:propose}
        $\Delta_+^{\min}(e) = F(\tilde{A}_e) - F(\tilde{A}_e \backslash e)$\;\label{alg:occ:deltaplusmin}
        $\Delta_+^{\max}(e) = F(\hat{A}_e   \cup e) - F(\hat{A}_e)$\;\label{alg:occ:deltaplusmax}
        $\Delta_-^{\min}(e) = F(\tilde{B}_e) - F(\tilde{B}_e \cup e)$\;\label{alg:occ:deltaminusmin}
        $\Delta_-^{\max}(e) = F(\hat{B}_e   \backslash e) - F(\hat{B}_e)$\;\label{alg:occ:deltaminusmax}
        Draw $u_e \sim Unif(0,1)$\;
        \If {$u_e < \frac{[\Delta_+^{\min}(e)]_+}{[\Delta_+^{\min}(e)]_+ + [\Delta_-^{\max}(e)]_+}$}{\label{alg:occ:decisioninclude}
          result$~\leftarrow 1$
        }\ElseIf {$u_e > \frac{[\Delta_+^{\max}(e)]_+}{[\Delta_+^{\max}(e)]_+ + [\Delta_-^{\min}(e)]_+}$}{
          result$~\leftarrow -1$\;
        }\lElse{result$~\leftarrow fail$}
        \Return (result, $u_e$)
      \end{algorithm}

      \begin{algorithm}[H]
        \DontPrintSemicolon
        \caption{\occ{}: validate($e$, $i$, $u_e$, result)}
        \label{alg:occvalidate}
        % \WaitUntil $\forall j<i$, result$(j) \neq 0$\;\label{alg:occ:resultwait}
        \WaitUntil $\forall j < i$, processed$(j) = true$\;
        \If{result$~= fail$}{
          $\Delta_+^{\text{exact}}(e) = F(\hat{A} \cup e) - F(\hat{A})$\;\label{alg:occvalidate:deltaplus}
          $\Delta_-^{\text{exact}}(e) = F(\hat{B} \backslash e) - F(\hat{B})$\;\label{alg:occvalidate:deltaminus}
          \lIf {$u_e < \frac{[\Delta_+^{\text{exact}}(e)]_+}{[\Delta_+^{\text{exact}}(e)]_+ + [\Delta_-^{\text{exact}}(e)]_+}$}{\label{alg:occvalidate:decisioninclude}
            result$~\leftarrow 1$
          }\lElse{
            result$~\leftarrow -1$
          }
        }
        \lIf {result$~= 1$}{
          $\hat{A}(e)   \leftarrow 1$;\label{alg:occ:ahat}
          $\tilde{B}(e) \leftarrow 1$
        }\lElse{
          $\tilde{A}(e) \leftarrow 0$;
          $\hat{B}(e)   \leftarrow 0$\label{alg:occ:bhat}
        }
        processed$(i) = true$\;
      \end{algorithm}


    \end{minipage}



  \end{multicols}
  \label{fig:submax}
\end{figure}








\section{\hogwild{} Bidirectional Greedy Algorithm}
\xinghao{Provide more intuition for what the \hogwild{} algorithm is doing.}

Algorithm \ref{alg:hogwild} is the \hogwild{} parallel bidirectional greedy algorithm for unconstrained submodular maximization.\footnote{We present only the parallelized probabilistic versions of \cite{buchbinder2012}. Both parallel algorithms can be easily extended to the deterministic version of \cite{buchbinder2012}; the \hogwild{} algorithm can also be extended to the multilinear version of \cite{buchbinder2012}.}
The \hogwild{} algorithm closely resembles the serial algorithm, but the elements $e \in V$ are no longer processed in a fixed order.  Thus, the sets $A, B$ are replaced by ``bounds'' $\hat{A}, \hat{B}$, where $\hat{A}$ is a subset of the ``true'' $A$ and $\hat{B}$ is a superset of the ``actual'' $B$ on each iteration.
These bounding sets allow us to compute bounds $\Delta_{+}^{\max}, \Delta_{-}^{\max}$ which approximate $\Delta_{+}, \Delta_{-}$ from the serial algorithm.
We now formalize this idea.

We order the elements $e \in V$ according to the time at which Algorithm \ref{alg:hogwild} line \ref{alg:hogwild:time} is executed.
Let $\iota(e)$ be the position of $e$ in this total ordering on elements.
This ordering allows us to define monotonically non-decreasing sets $A^i = \{e' : e' \in A, \iota(e') < i\}$, and monotonically non-increasing sets $B^i = A^i \cup \{e': \iota(e') \geq i\}$.
These ``true'' sets $A^i, B^i$ provide a serialization against which we can compare the \hogwild{} algorithm; in this serialization, Algorithm \ref{alg:submax} computes:
\begin{align*}
  \Delta_{+}       (e) &= F(A^{\iota(e)-1}\cup i) - F(A^{\iota(e)-1}),
& \Delta_{-}       (e) &= F(B^{\iota(e)-1}\backslash e) - F(B^{\iota(e)-1}) \, .
\end{align*}

Note that in Algorithm \ref{alg:hogwild}, lines \ref{alg:hogwild:deltaadd} and \ref{alg:hogwild:deltarem} may be executed in parallel with lines \ref{alg:hogwild:add} and \ref{alg:hogwild:rem}.
Hence, $\Delta_+^{\max}(e)$ and $\Delta_-^{\max}(e)$ (lines \ref{alg:hogwild:deltaadd} and \ref{alg:hogwild:deltarem}) may be computed with conflicting versions of $\hat{A}$ and $\hat{B}$.
Denoting these versions by $\hat{A}_e$ and $\hat{B}_e$, Algorithm \ref{alg:hogwild} computes:
\begin{align*}
  \Delta_{+}^{\max}(e) &= F(\hat{A}_e\cup e) - F(\hat{A}_e),
& \Delta_{-}^{\max}(e) &= F(\hat{B}_e\backslash e) - F(\hat{B}_e) \, .
\end{align*}

The next lemma shows that $\hat{A}_e, \hat{B}_e$ are bounding sets for the serialization's sets $A^{\iota(e)-1}, B^{\iota(e)-1}$.

\begin{restatable}{lem}{lemhogsetbound}\label{lem:hog:set_bound}
In the \hogwild{} algorithm, for any $e\in V$, $\hat{A}_e \subseteq A^{\iota(e)-1}$, and $\hat{B}_e \supseteq B^{\iota(e)-1}$.
\end{restatable}

\begin{cor}\label{cor:hog:delta_bound}
Submodularity of $F$ implies for \hogwild{}
$\Delta_{+}(e) \leq \Delta_{+}^{\max}(e)$, and
$\Delta_{-}(e) \leq \Delta_{-}^{\max}(e)$.
\end{cor}

The error in the \hogwild{} algorithm will depend on the tightness of the bounds in Corollary \ref{cor:hog:delta_bound}.
We analyze this error in \secref{sec:analysis:hogwild}.

% For some functions $F$, we can maintain sketches or statistics to aid the computation of $\Delta_+^{\max}$, $\Delta_-^{\max}$, and still obtain the bounds given in Corollary \ref{cor:hog:delta_bound}.
% In Appendix \ref{sec:sepsum}, we consider functions of separable sums, which are useful for applications such as document summarization \cite{lin11}.
%we consider functions of the form
%$F(X) = \sum_{l=1}^L g\left(\sum_{i\in X\cup S_l} w_l(i)\right) - \lambda\sum_{i\in X} v(i)$,
%where $S_l \subseteq V$ are (possibly overlapping) groups of elements in the ground set, $g$ is a non-decreasing concave scalar function, and $w_l(i)$ and $v(i)$ are non-negative scalar weights.










\section{Optimistic Concurrency Control for Bidirectional Greedy Algorithm}
\xinghao{Provide more intuition for what the \occ{} algorithm is doing.}

Algorithm \ref{alg:occ}\footnote{The synchronization required by the \occ{} algorithm can be further reduced, for example, by replacing the validation by a non-blocking function call.} is the \occ{} bidirectional greedy algorithm.
Unlike the \hogwild{} algorithm, the \occ{} algorithm ensures serial equivalence by maintaining four sets $\hat{A}$, $\tilde{A}$, $\hat{B}$, $\tilde{B}$, which serve as upper and lower bounds on $A$ and $B$.
Each thread can determine locally if a decision to include / exclude an element can be taken safely.
Otherwise, the validation process (Algorithm \ref{alg:occvalidate}) waits until it is certain about $A$, $B$ before proceeding.

The serialization order is given by $\iota(e)$, which is the value of $\iota$ at line \ref{alg:occ:time} of Algorithm \ref{alg:occ}.
We define $\hat{A}_e$, $\hat{B}_e$, $A^{\iota(e)-1}$, $B^{\iota(e)-1}$ as before with the \hogwild{} algorithm, and additionally let $\tilde{A}_e$ and $\tilde{B}_e$ be the vectors of $\tilde{A}$ and $\tilde{B}$ that are used in the computation of $\Delta_+^{\min}(e)$ and $\Delta_-^{\min}(e)$.
We will show that the outcome of Algorithm \ref{alg:occ} is equivalent to the sequential algorithm executed with ordering $\iota$.

\begin{restatable}{lem}{lemoccsetbound}\label{lem:occ:set_bound}
In the \occ{} algorithm, $\forall e\in V$,
$\hat{A}_e \subseteq A^{\iota(e)-1} \subseteq \tilde{A}_e \backslash e$, and $\hat{B}_e \supseteq B^{\iota(e)-1} \supseteq \tilde{B}_e \cup e$.
\end{restatable}

\begin{restatable}{lem}{lemoccvalidate}\label{lem:occ:validate}
In the \occ{} algorithm, during the validation process for element $e$, on Lines \ref{alg:occvalidate:deltaplus}, \ref{alg:occvalidate:deltaminus} of Algorithm \ref{alg:occvalidate}, we have $\hat{A} = A^{\iota(e)-1}$ and $\hat{B} = B^{\iota(e)-1}$.
\end{restatable}

Algorithm \ref{alg:occ} computes
\begin{align*}
  \Delta_+^{\min}(e) &= F(\tilde{A}_e) - F(\tilde{A}_e \backslash e),
& \Delta_+^{\max}(e) &= F(\hat{A}_e \cup e) - F(\hat{A})\\
  \Delta_-^{\min}(e) &= F(\tilde{B}_e) - F(\tilde{B}_e \cup e),
& \Delta_-^{\max}(e) &= F(\hat{B}_e \backslash e) - F(\hat{B}).
\end{align*}

\begin{cor}\label{cor:occ:delta_bound} Submodularity of $F$ implies that the $\Delta$'s computed by Algorithm \ref{alg:occ} satisfy: $\Delta_+^{\min}(e) \leq \Delta_+(e) = \Delta_+^{\text{exact}}(e) \leq \Delta_+^{\max}(e)$, and $\Delta_-^{\min}(e) \leq \Delta_-(e) = \Delta_+^{\text{exact}}(e) \leq \Delta_-^{\max}(e)$.
\end{cor}

% Like the \hogwild{} algorithm, the \occ{} algorithm can maintain statistics / sketches when dealing with functions of the form $F(X) = \sum_{l=1}^L g\left(\sum_{i\in X\cup S_l} w_l(i)\right) - \lambda\sum_{i\in X} v(i)$.
% We leave this discussion to Appendix \ref{sec:sepsum}.















\section{Analysis of Algorithms \label{sec:analysis}}

\xinghao{What is the purpose of this section?
We want to say that we have a choice between a slower but serially equivalent algorithm and a faster algorithm which is not serially equivalent.
Nevertheless, the slower algorithm is not too slow, and the approximation guarantee of the algorithm that is not serially equivalent is not too weak.}


\subsection{Approximation of \hogwild{} bidirectional greedy \label{sec:analysis:hogwild}}
Let $F$ be submodular and non-negative.
We assume for each $e$, there exists $\rho_e \geq 0$ such that
$\Delta_+^{\max}(e) - \rho_e \leq \Delta_+(e)$, and
$\Delta_-^{\max}(e) - \rho_e \leq \Delta_-(e)$.
This is possible, for example, by defining
\begin{align*}
\rho_e
&&:=&&&   \max\{\Delta_+^{\max}(e) - \Delta_+(e), \Delta_-^{\max}(e) - \Delta_-(e)\}\\
&&\leq&&& \max_{S,T\subseteq V} \{[F(S\cup e) - F(S)] - [F(S \cup T \cup e) - F(S \cup T)]\}
%&\leq F(e) - F(\emptyset) - F(V) + F(V\backslash e)\\
%&\leq F(e)\left(1 - \frac{F(V) - F(V\backslash e)}{F(e)}\right)\\
&\leq&& F(e)\kappa_F
\end{align*}
where $\kappa_F$ is the total curvature of $F$.
Summing over $e$ then gives us $\sum_e \rho_e \leq \kappa_F\sum_e F(e)$.

\begin{restatable}{thm}{thmrandomapprox}\label{thm:randomapprox} Let $F$ be a non-negative (monotone or non-monotone) submodular function.
The \hogwild{} bidirectional greedy algorithm solves the unconstrained problem $\max_{A\subset V} F(A)$ with approximation
$
E[F(A_{hog})] \geq \frac{1}{2}F^* - \frac{1}{4}\sum_{i=1}^n E[\rho_i]$,
where $A_{hog}$ is the output of the algorithm, $F^*$ is the optimal value, and $\rho_i$ is a random variable such that $\rho_i \geq \Delta_+^{\max}(i) - \Delta_+(i)$ and $\rho_i \geq \Delta_-^{\max}(i) - \Delta_-(i)$.
\end{restatable}

We prove the theorem in Appendix \ref{app:proofhogwild}.



\textbf{Example: max graph cut.}
Assuming bounded delay of $\tau$ and edges with unit weight, we can bound $\sum_i E[\rho_i] \leq 2\tau\frac{\text{\#edges}}{2N}$
The approximation of the \hogwild{} algorithm is then $E[F(A^n)] \geq = \frac{1}{2} F(OPT) - \tau\frac{\#\text{edges}}{2N}$.
In sparse graphs, the \hogwild{} algorithm is off by a small additional term, which albeit grows linearly in $\tau$.
In a complete graph, $F^* = \frac{1}{2}\#\text{edges}$, so $E[F(A^n)] \geq F^*\left(\frac{1}{2} - \frac{\tau}{N}\right)$, which makes it possible to scale $\tau$ linearly with $N$ while retaining the same approximation factor.


\textbf{Example: set cover.}
Consider the simple set cover function,
$F(A) = \sum_{l=1}^L \min(1,|A\cap S_l|) - \lambda|A| = |\{l: A\cap S_l \neq\emptyset\}| - \lambda|A|$,
with $0 < \lambda \leq 1$.
We assume that there is some bounded delay $\tau$.
Suppose also the $S_l$'s form a partition, so each element $e$ belongs to exactly one set.
Then, $\sum_e E[\rho_e] \geq \tau + L(1-\lambda^\tau)$, which is linear in $\tau$ but independent of $N$.



\subsection{Correctness of \occ{}}
\begin{thm} \occ{} bidirectional greedy is serially equivalent to bidirectional greedy.
\end{thm}
\begin{proof}
We will denote by $A_{seq}^i$, $B_{seq}^i$ the sets generated by the sequential algorithm, reserving $A^i$, $B^i$ for sets generated by the \occ{} algorithm.
It suffices to show by induction that $A_{seq}^i = A^i$ and $B_{seq}^i = B^i$.
For the base case, $A^0 = \emptyset = A_{seq}^0$, and $B^0 = V = B_{seq}^0$.
Consider any element $e$.
The \occ{} algorithm includes $e \in A$ iff $u_e < [\Delta_+^{\min}(e)]_+ ([\Delta_+^{\min}(e)]_+ + [\Delta_-^{\max}(e)]_+)^{-1}$ on Algorithm \ref{alg:occ} Line \ref{alg:occ:decisioninclude} or $u_e < [\Delta_+^{\text{exact}}(e)]_+ ([\Delta_+^{\text{exact}}(e)]_+ + [\Delta_-^{\text{exact}}(e)]_+)^{-1}$ on Algorithm \ref{alg:occvalidate} Line \ref{alg:occvalidate:decisioninclude}.
In both cases, Corollary \ref{cor:occ:delta_bound} implies $u_e < [\Delta_+(e)]_+ ([\Delta_+(e)]_+ + [\Delta_-(e)]_+)^{-1}$.
By induction, $A^{\iota(e)-1} = A_{seq}^{\iota(e)-1}$ and $B^{\iota(e)-1} = B_{seq}^{\iota(e)-1}$, so the threshold is exactly that computed by the sequential algorithm.
Hence, the \occ{} algorithm includes $e \in A$ iff the sequential algorithm includes $e \in A$.
(An analogous argument works for the case where $e$ is excluded from $B$.)

\end{proof}

As an immediate consequence, theoretical properties of the sequential algorithm are preserved by the \occ{} algorithm, including the approximation guarantees:

\begin{thm} Let $F$ be a non-negative (monotone or non-monotone) submodular function.
The \occ{} bidirectional greedy algorithm solves the unconstrained problem $\max_{A\subset V} F(A)$ with approximation
$E[F(A_{\occ{}})] \geq \frac{1}{2}F^*$,
where $A_{\occ{}}$ is the output of the algorithm, and $F^*$ is the optimal value.
\end{thm}




\subsection{Scalability of \occ{}}
Whenever an element is validated, it needs to wait for all earlier elements to be processed, and it blocks all later elements from being processed.
Each validation effectively constitutes a barrier to the parallel processing.
Hence, the scalability of the \occ{} algorithm is dependent on the number of validated elements.
Nevertheless, we show for a couple of example problems that the number of validated elements can be bounded.
Full details of the bounds are given in Appendix \ref{app:proofocc}.

\textbf{Example: max graph cut.}
Assume that there is a bounded delay $\tau$.
The expected number of validated elements is upper bounded by $\tau \frac{2\#edges}{N}$.

\textbf{Example: set cover.}
Assuming a bounded delay $\tau$ and non-overlapping sets $S_l$'s, the expected number of validated elements is upper bounded by $2\tau$.

















\section{Evaluation}

We implemented the parallel and sequential double greedy algorithms in Java / Scala.
Experiments were conducted on Amazon EC2 using one cc2.8xlarge machine, up to 16 threads, for 10 iterations.

We measured the runtime and speedup (ratio of runtime on 1 thread to runtime on $p$ threads).
For the \hogwild{} algorithm, we measured $F(A_{\hogwild})-F(A_{\seqalg})$, the difference between the objective value on the sets returned by \hogwild{} and the sequential algorithms.
We verified the correctness of the \occ{} algorithm by comparing the sets returned by the \occ{} and sequential algorithms, and also measured the fraction of elements that are validated by \occ{}.

%\begin{table}[h]
%\centering
%\begin{tabular}{|c|c|c|c|}\hline
%Graph & \# vertices & \# undirected edges & \# directed edges\\\hline\hline
%Google       &    875,713 &     3,852,985 &   4,563,235\\\hline
%BerkStan     &    685,230 &     6,649,470 &   7,600,595\\\hline
%Live Journal &  3,997,962 &    40,592,387 &  40,592,387\\\hline
%Orkut        &  3,072,441 &   117,181,608 & 117,181,608\\\hline
%Friendster   &  8,000,000 &   239,730,456 & 239,730,456\\\hline
%Friendster   & 10,000,000 &   625,279,786 & 625,279,786\\\hline
%Arabic-2005  & 22,744,080 & 1,107,806,146 & 631,153,669\\\hline
%UK-20005     & 39,459,925 & 1,566,054,250 & 921,345,078\\\hline
%\end{tabular}
%\end{table}
%\end{comment}

\begin{table}[h]
\centering\footnotesize
\begin{tabular}{|c|c|c|c|}\hline
Graph & \# vertices & \# edges & Description \\\hline\hline
Erdos-Renyi & 20,000,000 & $\approx 2 \times 10^6$ & Each edge is included with probability $5\times 10^{-6}$.\\\hline
\multirow{3}{*}{ZigZag}      & \multirow{3}{*}{25,000,000} &  \multirow{3}{*}{2,025,000,000} & Expander graph. The 81-regular zig-zag product \\
& & & between the Cayley graph on $\mathbb{Z}_{2500000}$ with generating \\
& & & set $\{\pm 1,\dots,\pm 5\}$, and the complete graph $K_{10}$.\\\hline
Friendster  & 10,000,000 &    625,279,786 & Subgraph of social network. \cite{snap}\\\hline
Arabic-2005 & 22,744,080 &    631,153,669 & 2005 crawl of Arabic web sites \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
UK-2005     & 39,459,925 &    921,345,078 & 2005 crawl of the .uk domain \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
IT-2004     & 41,291,594 &  1,135,718,909 & 2004 crawl of the .it domain \cite{BoVWFI, BRSLLP, BCSU3}. \\\hline
\end{tabular}
\caption{\footnotesize Synthetic and real graphs used in the evaluation of our parallel algorithms.}
\label{tab:graphstats}
\end{table}



We tested our parallel algorithms on the max graph cut and set cover problems with two synthetic graphs and three real datasets (Table \ref{tab:graphstats}).
Graphs were pre-processed to remove self-loops.
We found that vertices were typically indexed such that vertices close to each another in the graph were also close in their indices.
To reduce this dependency, we randomly permuted the ordering of vertices.
For the max graph cut problem, we removed directions on edges to obtain undirected graphs.
The set cover problem is reduced to a vertex cover on the directed graph.


\begin{figure}[ht]
  \centering
  \begin{tabular}{cccc}
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_relruntime.pdf}
			\caption{}
			\label{fig:relruntime}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_speedup_maxgraphcut.pdf}
			\caption{}
			\label{fig:speedup_maxgraphcut}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_speedup_setcover.pdf}
			\caption{}
			\label{fig:speedup_setcover}
	  \end{subfigure} \\
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_diffFA_maxgraphcut.pdf}
			\caption{}
			\label{fig:difffa_maxgraphcut}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_diffFA_setcover.pdf}
			\caption{}
			\label{fig:difffa_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/summary_validated_maxgraphcut.pdf}
			\caption{}
			\label{fig:validated_maxgraphcut}
	  \end{subfigure} \\
  \end{tabular}
  \caption{\footnotesize Experimental results.
  Fig \ref{fig:relruntime} -- runtime of the parallel algorithms as a ratio to that of the sequential algorithm. Each curve shows the runtime of a parallel algorithm on a particular graph for a particular function $F$.
  Fig \ref{fig:speedup_maxgraphcut}, \ref{fig:speedup_setcover} -- speedup (ratio of runtime on one thread to that on $p$ threads).
  Fig \ref{fig:difffa_maxgraphcut}, \ref{fig:difffa_setcover} -- \% difference between objective values of the sequential and \hogwild{} algorithms, i.e. $[F(A_{\hogwild{}}) / F(A_{\seqalg}) - 1] \times 100\%$.
  Fig \ref{fig:validated_maxgraphcut} -- percentage of elements validated by the \occ{} algorithm on the max graph cut problem.
  }
\label{fig:results_quality}
\end{figure}


Due to space constraints, we only present part of our results in Figure \ref{fig:results_quality}, deferring full results to Appendix \ref{app:exptresults}.
\textbf{Runtime, Speedup:} Both parallel algorithms are faster than the sequential algorithm with three or more threads, and show good speedup properties as more threads are added ($\sim$ 10x or more for all graphs and both functions).
\textbf{Objective value:} The objective value of the \hogwild{} algorithm decreases with the number of threads, but differs from the sequential objective value by less than $0.01\%$.
\textbf{Validations:} The \occ{} algorithm validates more elements as threads are added, but less than 0.015\% are validated with 16 threads, which has negligible effect on the runtime / speedup.

\subsection{Adversarial ordering}

To highlight the philosophical differences between the two parallel algorithms, we conducted experiments on a ring Cayley graph on $\mathbb{Z}_{10^6}$ with generating set $\{\pm 1,\dots, \pm 1000\}$.
The algorithms are presented with an adversarial ordering, with permutation, so vertices close in the ordering are adjacent to one another, and tend to be processed concurrently.
This causes \hogwild{} to make more mistakes, and \occ{} to face more uncertainty.

As Figure \ref{fig:results_adversarial} shows, \occ{}  sacrifices speed to ensure serial equivalence, eventually validating $>90\%$ of elements.
On the other hand, \hogwild{} focuses on speed, resulting in faster runtime, but delivering an objective value $F(A_{\hogwild{}})$ that is only $20\%$ of $F(A_{Seq2G})$.
For the set cover problem, we maintain statistics that are updated atomically by both algorithms.
The adversarial ordering forces more concurrent atomic updates, and hence, \hogwild{} does not achieve good speedup.
\footnote{We could have reduced coordination by computing $F$ directly, but doing so would result in longer runtimes.}

\begin{figure}[ht]
  \centering
  \begin{tabular}{cccc}
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/runtime_ring_setcover.pdf}
			\caption{}
			\label{fig:runtime_ring_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/speedup_ring_setcover.pdf}
			\caption{}
			\label{fig:speedup_ring_setcover}
	  \end{subfigure} &
	  \begin{subfigure}[h]{0.30\textwidth}
	  	\includegraphics[width=130pt]{images/validateddiffFA_ring_setcover.pdf}
			\caption{}
			\label{fig:validateddiffFA_ring_setcover}
	  \end{subfigure} \\
  \end{tabular}
  \caption{\footnotesize Experimental results for ring graph on set cover problem.}
\label{fig:results_adversarial}
\end{figure}

% \xinghao{These experiments were conducted by using an atomic integer to select elements to process. We could instead use a partitioning scheme, which has 2 advantages. Firstly, there is less coordination -- for \hogwild{}, we essentially have no coordination.
% Secondly, when faced with an adversarial ordering, the partitioning scheme allows big jumps / re-orderings, which reduces the number of validations and \textit{increases} the objective value of both parallel algorithms.}

We point out by using a partitioning scheme, it is possible to avoid the problems caused by the adversarial ordering, and to improve scalability.
Nevertheless, we present results that do use the partitioning scheme, so as to better highlight the differences between the two parallel algorithms.









\section{Related Work}
\textbf{Similar approach, different problem: } \occ{} DP-means \cite{pan2013}; \hogwild{} SGD \cite{Recht11}; \hogwild{} LDA \cite{Ahmed12} / parameter servers \cite{li2013, ho2013}

\textbf{Similar problem, different approach: } distributed greedy submodular maximization for monotone functions \cite{Mirzasoleiman2013}






\section{Discussions}

Conclusion: \xinghao{link back to intro, motivation}; we present two approaches to parallelizing unconstrained submodular maximization, which allows one to choose between speed and tight approximation guarantees.

Future work: constrained maximization, minimization; distributed setting, where communication costs and delays are higher, and function evaluations are challenging.


{\footnotesize
%\subsection*{Acknowledgments}
%This research is supported in part by NSF CISE Expeditions award CCF-1139158 and DARPA XData Award FA8750-12-2-0331, and  gifts from Amazon Web Services, Google, SAP,  Blue Goji, Cisco, Clearstory Data, Cloudera, Ericsson, Facebook, General Electric, Hortonworks, Intel, Microsoft, NetApp, Oracle, Samsung, Splunk, VMware and Yahoo!.
%This material is also based upon work supported in part by the Office of
%Naval Research under contract/grant number N00014-11-1-0688.
%X. Pan's work is also supported in part by a DSO National Laboratories Postgraduate Scholarship.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
%\nocite{langley00}

\bibliographystyle{unsrtnat}
\bibliography{references_arxiv}
}

\newpage
\appendix



% \input{sepsums}

\newpage\input{algoproofs}

\newpage\input{hogwildproof}

\newpage\input{occproof}

\newpage\section{Lemma}
\begin{lem}\label{lem:sumbinomial} $\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j} = {a+1 \choose b+1}$.
\end{lem}
\begin{proof}
\begin{align*}
&\sum_{k=t}^{a-b+t} {k-j \choose t-j} {a-k+j \choose b-t+j}\\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose t-j} {a-k'-t+j \choose b-t+j} \\
&= \sum_{k'=0}^{a-b} {k'+t-j \choose k'} {a-k'-t+j \choose a-b-k'} & \text{(symmetry of binomial coeff.)}\\
&= (-1)^{a-b}\sum_{k'=0}^{a-b} {-t+j-1 \choose k'} {-b+t-j-1 \choose a-b-k'} & \text{(upper negation)}\\
&= (-1)^{a-b} {-b-2 \choose a-b} & \text{(Chu-Vandermonde's identity)}\\
&= {a+1 \choose a-b} & \text{(upper negation)}\\
&= {a+1 \choose b+1} & \text{(symmetry of binomial coeff.)}\\
\end{align*}
\end{proof}

\newpage\input{exptresults}



\end{document}

% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz,
% slightly modified from the 2009 version by Kiri Wagstaff and
% Sam Roweis's 2008 version, which is slightly modified from
% Prasad Tadepalli's 2007 version which is a lightly
% changed version of the previous year's version by Andrew Moore,
% which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
